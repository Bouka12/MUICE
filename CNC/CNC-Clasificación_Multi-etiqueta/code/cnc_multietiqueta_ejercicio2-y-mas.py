# -*- coding: utf-8 -*-
"""CNC_MultiEtiqueta_Ecercice2mas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xegH6tQcE-nvbXnTjkdotKiKwEMgwFeY

**EJERCICIO 2**
Familiarízate con los datasets presentes en Scikit-multilearn y con los que trabajaremos
durante las próximas sesiones. ¿Qué información representan?. Concretamente, dispone de
los siguientes 17 datasets: {'scene', 'Corel5k', 'bibtex', 'enron', 'rcv1subset5', 'tmc2007_500',
'rcv1subset3', 'rcv1subset1', 'delicious', 'rcv1subset4', 'genbase', 'birds', 'emotions',
'rcv1subset2', 'mediamill', 'medical', 'yeast'}.
"""

#pip install scikit-multilearn
#!pip install liac-arff

import skmultilearn
from skmultilearn.dataset import load_dataset
from skmultilearn.dataset import load_dataset

# List of available datasets
datasets_list = ['scene', 'Corel5k', 'bibtex', 'enron', 'rcv1subset5', 'tmc2007_500',
                 'rcv1subset3', 'rcv1subset1', 'delicious', 'rcv1subset4', 'genbase',
                 'birds', 'emotions', 'rcv1subset2', 'mediamill', 'medical', 'yeast']

# Loop through each dataset and explore its characteristics
for dataset_name in datasets_list:
    # Load the dataset
    X, y, feature_names, label_names = load_dataset(dataset_name, 'undivided')

    # Basic dataset information
    print(f"Dataset: {dataset_name}")
    print(f"Number of instances: {X.shape[0]}")
    print(f"Number of features: {X.shape[1]}")
    print(f"Number of labels: {y.shape[1]}")


""" **Ejecricio 3**: Escribe un script en Python (car.py) que calcule para cada uno de los datasets de los dos
ejercicios anteriores las siguientes medidas de caracterización de cada dataset (como mínimo
los estadísticos vistos en teoría) y que las muestra en pantalla de forma ordenada:
a. number of instances (n)
b. number of attributes (f)
c. number of labels (l)
d. cardinality (car)
e. density (den)
f. diversity (div, represents the percentage of labelsets present in the dataset divided by
the number of possible labelsets)
g. average Imbalance Ratio per label (avgIR, measures the average degree of imbalance
of all labels, the greater avgIR, the greater the imbalance of the dataset)
h. ratio of unconditionally dependent label pairs by chi-square test (rDep, measures the
"""

# Ejercicio 3
from skmultilearn.dataset import load_dataset
from itertools import combinations
import numpy as np
from scipy.stats import chi2_contingency

# Define functions for calculations
def number_of_instances(X):
    return X.shape[0]

def number_of_attributes(X):
    return X.shape[1]

def number_of_labels(y):
    return y.shape[1]

# Function to calculate cardinality
def calculate_cardinality(y):
    return np.sum(y)

# Function to calculate density
def calculate_density(y):
    return np.mean(y)

# Function to calculate diversity
def calculate_diversity(y):
    unique_label_sets = set(map(tuple, y.toarray()))  # Convert lil_matrix to array for set operations
    possible_label_sets = 2 ** y.shape[1]
    return len(unique_label_sets) / possible_label_sets

# Function to calculate imbalance ratio for a label
def calculate_imbalance_ratio(y):
    label_counts = np.sum(y, axis=0)
    return np.mean(label_counts) / np.max(label_counts)

# Function to convert label sets to integers
def label_sets_to_integers(y):
    return y.dot(1 << np.arange(y.shape[-1] - 1, -1, -1))

# Function to calculate ratio of unconditionally dependent label pairs using chi-square test
def calculate_dependent_label_pairs(y):
    label_pairs = list(combinations(range(y.shape[1]), 2))
    dependent_pairs = 0

    for pair in label_pairs:
        contingency_table = np.zeros((2, 2), dtype=np.int64)

        label_1 = y[:, pair[0]].toarray().flatten()
        label_2 = y[:, pair[1]].toarray().flatten()

        contingency_table[0, 0] = np.sum((label_1 == 0) & (label_2 == 0))  # Both labels absent
        contingency_table[0, 1] = np.sum((label_1 == 0) & (label_2 == 1))  # Label 1 absent, Label 2 present
        contingency_table[1, 0] = np.sum((label_1 == 1) & (label_2 == 0))  # Label 1 present, Label 2 absent
        contingency_table[1, 1] = np.sum((label_1 == 1) & (label_2 == 1))  # Both labels present

        _, p, _, _ = chi2_contingency(contingency_table)

        if p < 0.01:  # Considering 99% confidence level
            dependent_pairs += 1

    total_pairs = len(label_pairs)
    return dependent_pairs / total_pairs
# List of available datasets
datasets_list = ['scene', 'Corel5k', 'bibtex', 'enron', 'rcv1subset5', 'tmc2007_500',
                 'rcv1subset3', 'rcv1subset1', 'delicious', 'rcv1subset4', 'genbase',
                 'birds', 'emotions', 'rcv1subset2', 'mediamill', 'medical', 'yeast']

def car(datasets_list):
# Iterate through datasets and calculate measures
  for dataset_name in datasets_list:
    # Load the dataset
      X, y, _, _ = load_dataset(dataset_name, 'undivided')

      # Calculate measures
      n_instances = number_of_instances(X)
      n_attributes = number_of_attributes(X)
      n_labels = number_of_labels(y)
      car = calculate_cardinality(y)
      den = calculate_density(y)
      div = calculate_diversity(y)
      avgIR = calculate_imbalance_ratio(y)
      rDep = calculate_dependent_label_pairs(y)

      # Print results
      print(f"Dataset: {dataset_name}")
      print(f"Number of instances (n): {n_instances}")
      print(f"Number of attributes (f): {n_attributes}")
      print(f"Number of labels (l): {n_labels}")
      print(f"Cardinality (car): {car}")
      print(f"Density (den): {den}")
      print(f"Diversity (div): {div}")
      print(f"Average Imbalance Ratio per label (avgIR): {avgIR}")
      print(f"Ratio of unconditionally dependent label pairs (rDep): {rDep}")
      print("\n")  # Add a separator between datasets
car(datasets_list)
